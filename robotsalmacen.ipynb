{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikelalvarezbejarano/sis420/blob/main/robotsalmacen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybe8CY07zp7D"
      },
      "source": [
        "## Scenario - Robots in a Warehouse\n",
        "Una empresa de comercio electrónico en crecimiento está construyendo un nuevo almacén y le gustaría que todas las operaciones de recolección en el nuevo almacén fueran realizadas por robots de almacén.\n",
        "\n",
        "En el contexto del almacenamiento del comercio electrónico, la \"recogida\" es la tarea de reunir artículos individuales de varias ubicaciones del almacén para cumplir con los pedidos de los clientes.\n",
        "Después de recoger los artículos de los estantes, los robots deben llevarlos a una ubicación específica dentro del almacén donde se pueden empaquetar para su envío.\n",
        "\n",
        "Para garantizar la máxima eficiencia y productividad, los robots deberán aprender el camino más corto entre el área de embalaje del artículo y todas las demás ubicaciones dentro del almacén donde los robots pueden viajar.\n",
        "\n",
        "¡Utilizaremos Q-learning para realizar esta tarea!\n",
        "Importar bibliotecas requeridas\n",
        "\n",
        "#### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfdhGGMsw1H7"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq-QPfDnx4Fo"
      },
      "source": [
        "## Define the Environment\n",
        "El entorno se compone de estados, acciones y recompensas. Los estados y las acciones son entradas para el agente de IA de Q-learning, mientras que las posibles acciones son las salidas del agente de IA.\n",
        "\n",
        "Estados\n",
        "Los estados del entorno son todas las ubicaciones posibles dentro del almacén. Algunas de estas ubicaciones son para almacenar artículos (cuadrados negros), mientras que otras ubicaciones son pasillos que el robot puede usar para recorrer el almacén (cuadrados blancos). El cuadrado verde indica el área de embalaje y envío del artículo.\n",
        "\n",
        "¡Los cuadrados negro y verde son estados terminales!\n",
        "\n",
        "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)\n",
        "\n",
        "El objetivo del agente de IA es aprender el camino más corto entre el área de embalaje del artículo y todas las demás ubicaciones del almacén donde el robot puede viajar.\n",
        "\n",
        "Como se muestra en la imagen de arriba, hay 121 estados (ubicaciones) posibles dentro del almacén. Estos estados están organizados en una cuadrícula que contiene 11 filas y 11 columnas. Por lo tanto, cada ubicación puede identificarse por su índice de fila y columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AdpFVfy6ya9"
      },
      "source": [
        "#define the shape of the environment (i.e., its states)\n",
        "environment_rows = 12\n",
        "environment_columns = 12\n",
        "\n",
        "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a)\n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
        "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
        "#each state (see next cell for a description of possible actions).\n",
        "#The value of each (state, action) pair is initialized to 0.\n",
        "q_values = np.zeros((environment_rows, environment_columns, 4))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07gGSNz07xtP"
      },
      "source": [
        "#### Actions\n",
        "\n",
        "Las acciones que están disponibles para el agente de IA son mover el robot en una de cuatro direcciones:\n",
        "* Arriba\n",
        "* Bien\n",
        "* Abajo\n",
        "* Izquierda\n",
        "\n",
        "¡Obviamente, el agente de IA debe aprender a evitar conducir hacia los lugares de almacenamiento de artículos (por ejemplo, estantes)!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z43QX_t080q3"
      },
      "source": [
        "#define actions\n",
        "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "actions = ['up', 'right', 'down', 'left']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25vn4VKw2as"
      },
      "source": [
        "#### Rewards\n",
        "El último componente del entorno que debemos definir son las **recompensas**.\n",
        "\n",
        "Para ayudar al agente de IA a aprender, a cada estado (ubicación) en el almacén se le asigna un valor de recompensa.\n",
        "\n",
        "El agente puede comenzar en cualquier cuadrado blanco, pero su objetivo es siempre el mismo: ***maximizar sus recompensas totales***.\n",
        "\n",
        "Las recompensas negativas (es decir, **castigos**) se utilizan para todos los estados excepto el objetivo.\n",
        "* ¡Esto anima a la IA a identificar el *camino más corto* hacia la meta *minimizando sus castigos*!\n",
        "\n",
        "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map-rewards.png)\n",
        "\n",
        "Para maximizar sus recompensas acumulativas (minimizando sus castigos acumulativos), el agente de IA necesitará encontrar los caminos más cortos entre el área de embalaje del artículo (cuadrado verde) y todas las demás ubicaciones del almacén donde el robot puede viajar (cuadrados blancos). ). ¡El agente también deberá aprender a evitar chocar contra cualquiera de las ubicaciones de almacenamiento de artículos (cuadrados negros)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJu7XsLXw62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762322d5-feec-4a15-f856-54dc7ad4b539"
      },
      "source": [
        "#Create a 2D numpy array to hold the rewards for each state.\n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
        "rewards = np.full((environment_rows, environment_columns), -100.)\n",
        "rewards[0, 5] = 100. #set the reward for the packaging area (i.e., the goal) to 100\n",
        "\n",
        "#define aisle locations (i.e., white squares) for rows 1 through 9\n",
        "aisles = {} #store locations in a dictionary\n",
        "aisles[1] = [i for i in range(1, 10)]\n",
        "aisles[2] = [1, 7, 9]\n",
        "aisles[3] = [i for i in range(1, 8)]\n",
        "aisles[3].append(9)\n",
        "aisles[4] = [3, 7]\n",
        "aisles[5] = [i for i in range(11)]\n",
        "aisles[6] = [5]\n",
        "aisles[7] = [i for i in range(1, 10)]\n",
        "aisles[8] = [3, 7]\n",
        "aisles[9] = [i for i in range(11)]\n",
        "\n",
        "#set the rewards for all aisle locations (i.e., white squares)\n",
        "for row_index in range(1, 10):\n",
        "  for column_index in aisles[row_index]:\n",
        "    rewards[row_index, column_index] = -1.\n",
        "\n",
        "#print rewards matrix\n",
        "for row in rewards:\n",
        "  print(row)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100. -100.]\n",
            "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100. -100.]\n",
            "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100. -100.]\n",
            "[  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100. -100.]\n",
            "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100. -100.]\n",
            "[  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n",
            "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFEor01iCCin"
      },
      "source": [
        "## Train the Model\n",
        "Our next task is for our AI agent to learn about its environment by implementing a Q-learning model. The learning process will follow these steps:\n",
        "1. Choose a random, non-terminal state (white square) for the agent to begin this new episode.\n",
        "2. Choose an action (move *up*, *right*, *down*, or *left*) for the current state. Actions will be chosen using an *epsilon greedy algorithm*. This algorithm will usually choose the most promising action for the AI agent, but it will occasionally choose a less promising option in order to encourage the agent to explore the environment.\n",
        "3. Perform the chosen action, and transition to the next state (i.e., move to the next location).\n",
        "4. Receive the reward for moving to the new state, and calculate the temporal difference.\n",
        "5. Update the Q-value for the previous state and action pair.\n",
        "6. If the new (current) state is a terminal state, go to #1. Else, go to #2.\n",
        "\n",
        "This entire process will be repeated across 1000 episodes. This will provide the AI agent sufficient opportunity to learn the shortest paths between the item packaging area and all other locations in the warehouse where the robot is allowed to travel, while simultaneously avoiding crashing into any of the item storage locations!\n",
        "\n",
        "#### Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnCfO5tVG0LJ"
      },
      "source": [
        "#define a function that determines if the specified location is a terminal state\n",
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
        "  if rewards[current_row_index, current_column_index] == -1.:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "#define a function that will choose a random, non-terminal starting location\n",
        "def get_starting_location():\n",
        "  #get a random row and column index\n",
        "  current_row_index = np.random.randint(environment_rows)\n",
        "  current_column_index = np.random.randint(environment_columns)\n",
        "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
        "  #(i.e., until the chosen state is a 'white square').\n",
        "  while is_terminal_state(current_row_index, current_column_index):\n",
        "    current_row_index = np.random.randint(environment_rows)\n",
        "    current_column_index = np.random.randint(environment_columns)\n",
        "  return current_row_index, current_column_index\n",
        "\n",
        "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
        "def get_next_action(current_row_index, current_column_index, epsilon):\n",
        "  #if a randomly chosen value between 0 and 1 is less than epsilon,\n",
        "  #then choose the most promising value from the Q-table for this state.\n",
        "  if np.random.random() < epsilon:\n",
        "    return np.argmax(q_values[current_row_index, current_column_index])\n",
        "  else: #choose a random action\n",
        "    return np.random.randint(4)\n",
        "\n",
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "  new_row_index = current_row_index\n",
        "  new_column_index = current_column_index\n",
        "  if actions[action_index] == 'up' and current_row_index > 0:\n",
        "    new_row_index -= 1\n",
        "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
        "    new_column_index += 1\n",
        "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
        "    new_row_index += 1\n",
        "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
        "    new_column_index -= 1\n",
        "  return new_row_index, new_column_index\n",
        "\n",
        "#Define a function that will get the shortest path between any location within the warehouse that\n",
        "#the robot is allowed to travel and the item packaging location.\n",
        "def get_shortest_path(start_row_index, start_column_index):\n",
        "  #return immediately if this is an invalid starting location\n",
        "  if is_terminal_state(start_row_index, start_column_index):\n",
        "    return []\n",
        "  else: #if this is a 'legal' starting location\n",
        "    current_row_index, current_column_index = start_row_index, start_column_index\n",
        "    shortest_path = []\n",
        "    shortest_path.append([current_row_index, current_column_index])\n",
        "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
        "    while not is_terminal_state(current_row_index, current_column_index):\n",
        "      #get the best action to take\n",
        "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "      #move to the next location on the path, and add the new location to the list\n",
        "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "      shortest_path.append([current_row_index, current_column_index])\n",
        "    return shortest_path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjl9niKEqONs"
      },
      "source": [
        "#### Train the AI Agent using Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N5BB0m0JHIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bce297a-f551-431d-db3f-0825f69919d4"
      },
      "source": [
        "#define training parameters\n",
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "#run through 1000 training episodes\n",
        "for episode in range(1000):\n",
        "  #get the starting location for this episode\n",
        "  row_index, column_index = get_starting_location()\n",
        "\n",
        "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "  while not is_terminal_state(row_index, column_index):\n",
        "    #choose which action to take (i.e., where to move next)\n",
        "    action_index = get_next_action(row_index, column_index, epsilon)\n",
        "\n",
        "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "\n",
        "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "    reward = rewards[row_index, column_index]\n",
        "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
        "\n",
        "    #update the Q-value for the previous state and action pair\n",
        "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "print('Training complete!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqQfjYdfyBh"
      },
      "source": [
        "## Get Shortest Paths\n",
        "Now that the AI agent has been fully trained, we can see what it has learned by displaying the shortest path between any location in the warehouse where the robot is allowed to travel and the item packaging area.\n",
        "\n",
        "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)\n",
        "\n",
        "Run the code cell below to try a few different starting locations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1YO3mj_oS2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d100c7-b85c-49af-8d93-26d06b5b7dca"
      },
      "source": [
        "#display a few shortest paths\n",
        "print(get_shortest_path(3, 9)) #starting at row 3, column 9\n",
        "print(get_shortest_path(5, 0)) #starting at row 5, column 0\n",
        "print(get_shortest_path(9, 5)) #starting at row 9, column 5"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 9], [2, 9], [1, 9], [1, 8], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
            "[[5, 0], [5, 1], [5, 2], [5, 3], [4, 3], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
            "[[9, 5], [9, 6], [9, 7], [8, 7], [7, 7], [7, 6], [7, 5], [6, 5], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWx7BsJxqrDv"
      },
      "source": [
        "#### Finally...\n",
        "It's great that our robot can automatically take the shortest path from any 'legal' location in the warehouse to the item packaging area. **But what about the opposite scenario?**\n",
        "\n",
        "Put differently, our robot can currently deliver an item from anywhere in the warehouse ***to*** the packaging area, but after it delivers the item, it will need to travel ***from*** the packaging area to another location in the warehouse to pick up the next item!\n",
        "\n",
        "Don't worry -- this problem is easily solved simply by ***reversing the order of the shortest path***.\n",
        "\n",
        "Run the code cell below to see an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKun8LInsas9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5136b71-da05-4f09-9909-8f9312db1a2b"
      },
      "source": [
        "#display an example of reversed shortest path\n",
        "path = get_shortest_path(5, 2) #go to row 5, column 2\n",
        "path.reverse()\n",
        "print(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 5], [1, 5], [1, 6], [1, 7], [2, 7], [3, 7], [4, 7], [5, 7], [5, 6], [5, 5], [5, 4], [5, 3], [5, 2]]\n"
          ]
        }
      ]
    }
  ]
}